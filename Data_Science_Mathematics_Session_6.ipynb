{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Mathematics\n",
    "# Principal Component Analysis\n",
    "# In-Class Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###1) Show that the following is true (remember that variance is the square of the standard deviation):\n",
    "\n",
    "> *cov(x,x)=var(x)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###2) Explain the benefits of dimensionality reduction on large data sets. In what ways might dimensionality reduction be detrimental? Recall from our Clustering lecture the \"Curse of Dimensionality.\" How might the utility of dimensionality reduction be explained in this context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large data sets, reducing dimensionality allows for easier and faster computation, especially when not all factors / features may be of equal interest or significance. \n",
    "\n",
    "In the Clustering lecture, we discussed the Curse of Dimensionality, which postulates that as the number of dimensions increases, the distance between an centroid and a given point in n-dimensional space begins to become equal (e.g. it becomes much more difficult to identify clusters based on meaningful factors and data can become \"overfit\", which is functionally useless).\n",
    "\n",
    "In this case, reducing dimensionality not only makes computation faster, but also increases the likelihood of useful clusters, which is very useful as a means of analyzing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###3) Calculate (by hand) the eigenvalues and the associated eigenvectors of matrix A:\n",
    "\n",
    "```\n",
    "| 3     0     0 |\n",
    "| -4    6     2 |\n",
    "| 16   -15   -5 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the equation for determining the eigenvalues (**L** / lambda) in Matrix A is *A - **L** x I* where *I* is the identity matrix. This results in the following:\n",
    "\n",
    "```\n",
    "| 3-L   0     0 |\n",
    "| -4   6-L    2 |\n",
    "| 16  -15  -5-L |\n",
    "```\n",
    "\n",
    "we solve for the determinant in order to find the eigenvalues:\n",
    "\n",
    "```(3-L)*((6-L)*(-5-L)-(-15*2))-0*(-4*(-5-L)-(16*2))+0*(-4*-15-16*(6-L)) = 0```\n",
    "\n",
    "simplified, this comes out to:\n",
    "\n",
    "`3*(6*-5 + -L*-5 + 6*-L + -L*-L + 30)-L(6*-5 + -L*-5 + 6*-L + -L*-L + 30) = 0`\n",
    "\n",
    "`3*6*-5 + 3*-L*-5 + 3*6*-L + 3*-L*-L + 3*30  -  L*6*-5 + -L*-L*-5 + -L*6*-L + -L*-L*-L + -L*30 = 0`\n",
    "\n",
    "`-90 + 15L + -18L + 3L^2 + 90 + 30L + -5L^2 + 6L^2 +-L^3 + -30L = 0`\n",
    "\n",
    "`-L^3 + 3L^2 + -5L^2 + 6L^2 + 15L + -18L + 30L + -30L + -90 + 90 = 0`\n",
    "\n",
    "`-L^3 + 4L^2 - 3L = 0`\n",
    "\n",
    "`L^3 - 4L^2 + 3L = 0`\n",
    "\n",
    "`L*(L^2 - 4L + 3) = 0`\n",
    "\n",
    "`L*(L - 1)(L - 3) = 0`\n",
    "\n",
    "Accordingly, **the eigenvalues are: 0, 1, 3**\n",
    "\n",
    "Plugging these back in to the equation, our eigenvectors come out as:\n",
    "\n",
    "Solving for the **eigenvector for L = 0:**\n",
    "\n",
    "```\n",
    "| 3-0   0     0 | |x|\n",
    "| -4   6-0    2 |*|y|\n",
    "| 16  -15  -5-0 | |z|\n",
    "```\n",
    "\n",
    "Simplifies to:\n",
    "\n",
    "```\n",
    "3x = 0\n",
    "-4x + 6y + 2z = 0\n",
    "16x - 15y - 5z = 0\n",
    "```\n",
    "\n",
    "We recognize from the first equation that **x=0** must be a solution, so we substitute that back in to the lower two equations, add them, and solve for **z**:\n",
    "\n",
    "```\n",
    "6y-15y + 2z-5z = 0\n",
    "-9y -3z = 0\n",
    "-3y - z = 0\n",
    "-3y = z\n",
    "```\n",
    "\n",
    "So our additional solutions are **y=-1** and **z=3**, making the **L=0 eigenvector {0,-1,3}**\n",
    "\n",
    "\n",
    "Solving for the **eigenvector for L = 1:**\n",
    "\n",
    "```\n",
    "| 3-1   0     0 | |x|\n",
    "| -4   6-1    2 |*|y|\n",
    "| 16  -15  -5-1 | |z|\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "| 3-3   0     0 | |x|\n",
    "| -4   6-3    2 |*|y|\n",
    "| 16  -15  -5-3 | |z|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###4) Compute (by hand) the determinant of matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a 3x3 matrix:\n",
    "\n",
    "```\n",
    "| a1 b1 c1 |\n",
    "| a2 b2 c2 |\n",
    "| a3 b3 c3 |\n",
    "```\n",
    "\n",
    "The determinant of that 3x3 matrix can be expressed as:\n",
    "\n",
    "```a1*[b2 c2 | b3 c3] - b1*[a2 c2 | a3 c3] + c1*[a2 b2 | a3 b3]```\n",
    "\n",
    "This can then be simplified to:\n",
    "\n",
    "```(a1*b2*c3 + b1*c2*a3 + c1*a2*b3) - (a3*b2*c1 + b3*c2*a1 + c3*a2*b1)```\n",
    "\n",
    "This, for matrix A is:\n",
    "\n",
    "```(3*6*-5 + 0*2*16 + 0*-4*-15)-(16*6*0 + -15*2*3 + -5*-4*0)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant: 0\n"
     ]
    }
   ],
   "source": [
    "x = ((3*6*-5) + (0*2*16) + (0*-4*-15))\n",
    "y = ((16*6*0) + (-15*2*3) + (-5*-4*0))\n",
    "print('Determinant:',x-y)\n",
    "\n",
    "#The determinant describes the area of a transformation.\n",
    "#In this case, it is 0.\n",
    "#This means the transformation resulted in a line or a dot, both of which have area 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###5) You are a data scientist at a three-letter agency. You have been following a group of suspected ISIL members on social media, and have derived 4 features from various profiles. You are developing a supervised learning algorith for identifying ISIL members based on these features, and need to project your data onto **two dimensions** for clustering analysis.\n",
    "\n",
    "###Do the following:\n",
    ">a) Derive a covariance matrix from this data set\n",
    "\n",
    ">b) Calculate the feature vector of eigenvalues from the covariance matrix\n",
    "\n",
    ">c) Project the data set into the appropriate principle component space\n",
    "\n",
    ">d) Assuming the class of each record is known, explain how this reduced data set could be used to derive a supervised learning algorith based on clustering\n",
    "\n",
    ">e) BONUS: Graph the 2D principle components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to your in-class handout for instructions.  You are going to do most of the coding yourself here.\n",
    "\n",
    "Read about this library here:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "First, let's import our relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to instantiate our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[5.1,3.5,1.4,0.2],\n",
    "[4.9,3.0,1.4,0.2],\n",
    "[4.7,3.2,1.3,0.2],\n",
    "[4.6,3.1,1.5,0.2],\n",
    "[5.0,3.6,1.4,0.2],\n",
    "[5.4,3.9,1.7,0.4],\n",
    "[4.6,3.4,1.4,0.3],\n",
    "[5.0,3.4,1.5,0.2],\n",
    "[4.4,2.9,1.4,0.2],\n",
    "[4.9,3.1,1.5,0.1],\n",
    "[5.4,3.7,1.5,0.2],\n",
    "[4.8,3.4,1.6,0.2],\n",
    "[4.8,3.0,1.4,0.1],\n",
    "[4.3,3.0,1.1,0.1],\n",
    "[5.8,4.0,1.2,0.2],\n",
    "[5.7,4.4,1.5,0.4],\n",
    "[5.4,3.9,1.3,0.4],\n",
    "[5.1,3.5,1.4,0.3],\n",
    "[5.7,3.8,1.7,0.3],\n",
    "[5.1,3.8,1.5,0.3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###a) Derive a covariance matrix from this data set\n",
    "\n",
    "Now, in the cell below, calculate your covariance matrix for the above data set:\n",
    "> c = np.cov(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.cov(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.75       4.42166667 4.35333333 4.16       4.69666667 4.86\n",
      "  4.215      4.595      3.965      4.49333333 5.03       4.38666667\n",
      "  4.415      4.105      5.58       5.35       5.01333333 4.67166667\n",
      "  5.16166667 4.72833333]\n",
      " [4.42166667 4.14916667 4.055      3.885      4.35833333 4.515\n",
      "  3.9075     4.28416667 3.7075     4.21       4.68333333 4.08333333\n",
      "  4.1375     3.81416667 5.18       4.93666667 4.645      4.34916667\n",
      "  4.81916667 4.37916667]\n",
      " [4.35333333 4.055      3.99       3.81333333 4.30333333 4.45333333\n",
      "  3.86166667 4.21166667 3.635      4.12       4.61       4.02\n",
      "  4.04833333 3.76166667 5.11333333 4.9        4.59333333 4.28166667\n",
      "  4.73166667 4.33166667]\n",
      " [4.16       3.885      3.81333333 3.65666667 4.11       4.25666667\n",
      "  3.68833333 4.03166667 3.485      3.95333333 4.40666667 3.85333333\n",
      "  3.88166667 3.59166667 4.86666667 4.66333333 4.37       4.08833333\n",
      "  4.52833333 4.135     ]\n",
      " [4.69666667 4.35833333 4.30333333 4.11       4.65       4.81\n",
      "  4.175      4.54166667 3.915      4.43333333 4.97333333 4.34\n",
      "  4.355      4.06166667 5.52       5.30666667 4.96333333 4.61833333\n",
      "  5.09833333 4.685     ]\n",
      " [4.86       4.515      4.45333333 4.25666667 4.81       4.97666667\n",
      "  4.31833333 4.70166667 4.055      4.59333333 5.14666667 4.49333333\n",
      "  4.51166667 4.20166667 5.70666667 5.48333333 5.13       4.77833333\n",
      "  5.27833333 4.845     ]\n",
      " [4.215      3.9075     3.86166667 3.68833333 4.175      4.31833333\n",
      "  3.74916667 4.07583333 3.5125     3.97666667 4.46333333 3.89666667\n",
      "  3.90583333 3.64583333 4.95333333 4.76666667 4.455      4.14416667\n",
      "  4.57416667 4.2075    ]\n",
      " [4.595      4.28416667 4.21166667 4.03166667 4.54166667 4.70166667\n",
      "  4.07583333 4.44916667 3.8425     4.35666667 4.86666667 4.25\n",
      "  4.27916667 3.96916667 5.38666667 5.16333333 4.83833333 4.5175\n",
      "  4.9975     4.57083333]\n",
      " [3.965      3.7075     3.635      3.485      3.915      4.055\n",
      "  3.5125     3.8425     3.3225     3.77       4.2        3.67\n",
      "  3.7025     3.4225     4.64       4.44       4.165      3.8975\n",
      "  4.3175     3.9375    ]\n",
      " [4.49333333 4.21       4.12       3.95333333 4.43333333 4.59333333\n",
      "  3.97666667 4.35666667 3.77       4.28       4.76       4.16\n",
      "  4.20333333 3.87666667 5.25333333 5.02       4.71333333 4.41666667\n",
      "  4.89666667 4.45666667]\n",
      " [5.03       4.68333333 4.61       4.40666667 4.97333333 5.14666667\n",
      "  4.46333333 4.86666667 4.2        4.76       5.32666667 4.64666667\n",
      "  4.67666667 4.34666667 5.90666667 5.66333333 5.30666667 4.94666667\n",
      "  5.46666667 5.00666667]\n",
      " [4.38666667 4.08333333 4.02       3.85333333 4.34       4.49333333\n",
      "  3.89666667 4.25       3.67       4.16       4.64666667 4.06666667\n",
      "  4.08333333 3.79       5.13333333 4.93333333 4.61333333 4.31\n",
      "  4.77       4.37      ]\n",
      " [4.415      4.1375     4.04833333 3.88166667 4.355      4.51166667\n",
      "  3.90583333 4.27916667 3.7025     4.20333333 4.67666667 4.08333333\n",
      "  4.12916667 3.80916667 5.16666667 4.93333333 4.635      4.34083333\n",
      "  4.81083333 4.3775    ]\n",
      " [4.105      3.81416667 3.76166667 3.59166667 4.06166667 4.20166667\n",
      "  3.64583333 3.96916667 3.4225     3.87666667 4.34666667 3.79\n",
      "  3.80916667 3.54916667 4.82666667 4.63333333 4.33833333 4.0375\n",
      "  4.4575     4.09083333]\n",
      " [5.58       5.18       5.11333333 4.86666667 5.52       5.70666667\n",
      "  4.95333333 5.38666667 4.64       5.25333333 5.90666667 5.13333333\n",
      "  5.16666667 4.82666667 6.58666667 6.31333333 5.92       5.49333333\n",
      "  6.05333333 5.56      ]\n",
      " [5.35       4.93666667 4.9        4.66333333 5.30666667 5.48333333\n",
      "  4.76666667 5.16333333 4.44       5.02       5.66333333 4.93333333\n",
      "  4.93333333 4.63333333 6.31333333 6.08666667 5.68333333 5.26333333\n",
      "  5.79333333 5.35333333]\n",
      " [5.01333333 4.645      4.59333333 4.37       4.96333333 5.13\n",
      "  4.455      4.83833333 4.165      4.71333333 5.30666667 4.61333333\n",
      "  4.635      4.33833333 5.92       5.68333333 5.32333333 4.935\n",
      "  5.435      5.00166667]\n",
      " [4.67166667 4.34916667 4.28166667 4.08833333 4.61833333 4.77833333\n",
      "  4.14416667 4.5175     3.8975     4.41666667 4.94666667 4.31\n",
      "  4.34083333 4.0375     5.49333333 5.26333333 4.935      4.59583333\n",
      "  5.07583333 4.64916667]\n",
      " [5.16166667 4.81916667 4.73166667 4.52833333 5.09833333 5.27833333\n",
      "  4.57416667 4.9975     4.3175     4.89666667 5.46666667 4.77\n",
      "  4.81083333 4.4575     6.05333333 5.79333333 5.435      5.07583333\n",
      "  5.61583333 5.12916667]\n",
      " [4.72833333 4.37916667 4.33166667 4.135      4.685      4.845\n",
      "  4.2075     4.57083333 3.9375     4.45666667 5.00666667 4.37\n",
      "  4.3775     4.09083333 5.56       5.35333333 5.00166667 4.64916667\n",
      "  5.12916667 4.7225    ]]\n"
     ]
    }
   ],
   "source": [
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###b) Calculate the feature vector of eigenvalues from the covariance matrix\n",
    "\n",
    "Now, in the cell below, calculate the eigenvectors and eigenvalues of the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.eig(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###c) Project the data set into the appropriate principle component space\n",
    "\n",
    "Now calculate the principal components (reduce to 2 dimensions).  First, you need to instantiate your PCA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the cell below, train your model on your dataset:\n",
    "> pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands can be used to get your explained variance ratios (percentage of variance explained by each of the selected components) and your dimensionally-reduced components.\n",
    "\n",
    "Print these values in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###d) Assuming the class of each record is known, explain how this reduced data set could be used to derive a supervised learning algorith based on clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###e) BONUS: Graph the 2D principle components\n",
    "\n",
    "Bonus: Figure out how to plot your principal components as a scatter plot:\n",
    "\n",
    "https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now save your output.  Go to File -> Print Preview and save your final output as a PDF.  Turn in to your Instructor, along with any additional sheets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
